Lecture: http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets

[2:00] Slide này nêu 2 nhược điểm của CNN:

Nhược điểm 1: "Too few level of structure" -> cái này thì rõ ràng rồi, mạng neuron của con người có hàng tỷ neurons và vô cùng phức tạp 
đến nỗi không ai hiểu được, mạng CNN phổ biến thì chỉ có vài mống layer. Mạnh nhất là mạng DenseNet có hơn 1k layers nhưng 
1k là quá khiêm tốn so với não người.
    
Nhược điểm 2: "need to group the neurons in each layer into capsules that do a lot of internal computation and then output a compact result."
Cái này thì mình không hiểu lắm là bác ấy định nói gì. Slide sau [2:50] bác ấy nói rằng một capsule thể hiện một 
"multi-dimensional entity of the type that the capsule detects", và "a capsule detects two things:"
-1. The probability that an object of that type is present.
-2. The generalized pose of the object which includes position, orientation, scale, deformation, velocity, color, etc...

Thật sự là đến đây mới chỉ nghe có 3 phút nhưng mình đã lại nhớ cái hồi học khóa NN của bác này trên Coursera: cảm giác rất nản vì không biết
bác ấy định nói gì. Kiểu cứ một mình bác chơi một kiểu từ vựng. Filter thì gọi quách là filter, feature map thì gọi quách là feature map.
Sao phải gọi là "capsule"? Mà "group the neurons into capsules that do a lot of internal computation" thì liên quan gì tới nhược điểm?
Với lại capsule tại sao lại vừa thể hiện một số (probability), vừa thể hiện một vector toán học (multi-dimensional entity), lại vừa thể hiện
features 2D (position, orientation, velocity), rồi lại đại lượng vật lý (scale, color)? Nói chung là khó hiểu. Sau 2 năm học đủ các thứ
trên đời thì bài giảng của bác Hinton này là khó nuốt nhất.

[8:28] On pooling layer: "The fact that it works so well is extremely unfortunate because that makes it harder to get rid of" :)) 
đúng là ngữ khí của bậc đại cao thủ, sẵn sàng phê phán tới cùng một kiến trúc thần thánh đã làm điên đảo thế giới. Những nghiên cứu gần đây
về Fully Convolutional Nets (FCN hoặc FCNN) đã chứng minh là nhận định của bác Hinton là hoàn toàn có cơ sở, khi mà pooling layer
thật ra là không cần thiết và có thể mô phỏng pooling bằng non-overlapping conv layer. Cách bác Hinton đặt vấn đề và diễn giải cho thấy
một nội công Cửu Dương thần công tinh thuần bá đạo của 1 người đã làm neural net từ những năm 87s.

[9:04] On activation: "The activations in the last hidden layer of a deep convnet are a percept", có thể dịch là "activations của 
lớp hidden layer cuối cùng của một mạng CNN là một sự nhận thức". Trong câu này có rất nhiều điểm đáng chú ý: lớp hidden cuối cùng là 
lớp nào? Là lớp convolution cuối cùng hay là lớp fully connected cuối cùng? Đặc biệt hơn cả là khái niệm "percept". Activations của
cái lớp cuối cùng này không chỉ là một đống số thực vô tri vô giác mà là một "nhận thức". 

[13:51] "So I'm gonna give you an example that illustrates the power of coordinate frame" => epic example, reminds me of TBBT every time :))
On a serious note, the example does have a very good point: each half of the tetrahedron can be seen as primitive features, and if it's
so hard for us to combine these two features together in a good way (e.g. come up with good coordinate frames)
, how could we expect coordinate-lacking ANN (especially CNN) to do that? At this point I start feel like CNN and RNN are our heros who have different
super powers, just like the Flash and Superman, yet they are not gods and they have weeknesses.

[23:48] Tổng kết Conclusion của argument 1: đến đây ta thấy rõ hơn là tại sao cái "nhận thức" lại liên kết tới ví dụ ở trên. Vì 
activations của neurons trong não người thể hiện một nhận thức, mà nhận thức ấy thay đổi cực nhiều theo hệ quy chiếu (frame) được nhúng
trong các vật thể xung quanh ta ("frames embedded in objects"), nên đây rõ ràng là điểm yếu của CNN khi một mạng CNN chỉ có thể phản 
ánh cái frame của người chụp ảnh tạo ra dataset cho nó mà thôi. Đây sẽ là một luận điểm quan trọng khi làm object segmentation trong occluded
environment, khi mà khung cảnh bị ảnh hưởng nặng nề bởi các vật thể khác. Hệ quả thú vị hơn là có thể trong tương lai nếu ANN có cơ chế
tốt hơn não người thì con người sẽ không thể verify hoặc analyze các thuật toán AI bằng manual error analysis được nữa, vì những gì AI
nhìn thấy sẽ khác con người nhìn thấy.

[38:00 - 39:xx] Đoạn này bác Hinton giành rất nhiều thời gian nói về routing. Ý tưởng của routing là layer ở dưới sẽ truyền thông tin lên layer 
ở trên (tính từ input image tới fully connected layer là từ dưới lên trên). Khi pooling thì có những thông tin quan trọng bị mất ở 
lớp pooling layer mà layer trên không thể "hỏi" trực tiếp layer ở dưới được (thêm 1 lý do để bác Hinton ghét pooling layer :))). 
Slide ở 40:11 tiếp theo "Proof of Concept" là cực kỳ quan trọng.

[40:11] "A proof of concept". Slide này có 2 điểm:
- "The aim is to demonstrate that it is possible to learn a shallow hierrachy of parts using supervised SGD with no hand-engineering of the parts."
Đây chính là tinh thần tối thượng của CNN. Điểm đặc biệt là ở chữ "shallow". Như thế nào thì đủ "shallow"? 
- "The design of the system forces it to extract the poses of parts and to recognize objects by using the agreement of pose predictions."
SIÊU LIÊN QUAN! Đây là điểm mà mình đã suy nghĩ từ lâu: "ép" layer của CNN phải học cái mình muốn như thế nào? Và ép tới mức nào thì không
đánh mất ý nghĩa của việc "tự học"? Đoạn text trong slide ở ngay dưới nêu một gợi ý:
    - But the system decides for itself what the parts are and what intrinsic coordinate frames to embed them.
Như vậy là ta sẽ ép cho CNN học "tư thế" (pose) của objects, nhưng cái tư thế ấy được mô tả trong hệ tọa độ nào thì ta không quan tâm, miễn sao đủ 
độ chi tiết để phân biệt object là được.

Đến đây cần phải nhớ lại cái trăn trở của mình khi đọc về tăng performance cho CNN, đấy là khả năng 
"overfit in the high layers and underfit in the low layers due to poor gradients backpropagation". Việc weights của một network được initialize
randomly và quá trình gradients backpropagation không kiểm soát tốt dẫn tới là cả network không học được gì. Một trong số những cách khắc phục 
mà mình đã nhìn thấy ở các model nổi tiếng gần đây như ResNet, DenseNet là việc họ có nhiều output! Theo một cách hiểu là giống như transfer learning,
một cái Neural net sẽ học nhiều task cùng một lúc và lớp output của các task đó sẽ gắn với các mức hidden layer khác nhau. Như vậy,
các layers ở dưới sẽ không chỉ nhận gradients từ layer ở trên mà còn nhận trực tiếp gradients của loss gắn vào nó. Ý tưởng là chỉ gắn thêm loss
cho layers ở dưới vì các layers ở dưới phải học được các features cơ bản nhất mà có ích cho nhiều tác vụ khác nhau.

[Sau đấy] Phần sau giải thích khá khó hiểu, lúc nào khỏe quay lại xem tiếp.
